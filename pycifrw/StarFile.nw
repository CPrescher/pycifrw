<<Copyright statement>>=
"""
1.This Software copyright \u00A9 Australian Synchrotron Research Program Inc, ("ASRP").

2.Subject to ensuring that this copyright notice and licence terms
appear on all copies and all modified versions, of PyCIFRW computer
code ("this Software"), a royalty-free non-exclusive licence is hereby
given (i) to use, copy and modify this Software including the use of
reasonable portions of it in other software and (ii) to publish,
bundle and otherwise re-distribute this Software or modified versions
of this Software to third parties, provided that this copyright notice
and terms are clearly shown as applying to all parts of software
derived from this Software on each occasion it is published, bundled
or re-distributed.  You are encouraged to communicate useful
modifications to ASRP for inclusion for future versions.

3.No part of this Software may be sold as a standalone package.

4.If any part of this Software is bundled with Software that is sold,
a free copy of the relevant version of this Software must be made
available through the same distribution channel (be that web server,
tape, CD or otherwise).

5.It is a term of exercise of any of the above royalty free licence
rights that ASRP gives no warranty, undertaking or representation
whatsoever whether express or implied by statute, common law, custom
or otherwise, in respect of this Software or any part of it.  Without
limiting the generality of the preceding sentence, ASRP will not be
liable for any injury, loss or damage (including consequential loss or
damage) or other loss, loss of profits, costs, charges or expenses
however caused which may be suffered, incurred or arise directly or
indirectly in respect of this Software.

6. This Software is not licenced for use in medical applications.
"""

@ \section{Introduction}

This file implements a general STAR reading/writing utility.  The basic
objects ([[StarFile/StarBlock]]) read and write syntactically correct STAR files
including save frames.

The [[StarFile]] class is initialised with either no arguments (a new STAR file) 
or with the name of an already existing STAR file.  Data items are 
accessed/changed/added using the python mapping type ie to get 
[[dataitem]] you would type [[value = cf[blockname][dataitem]]]. 

The methods available for the StarFile type are:
\begin{enumerate}
\item [[ReadStar(filestream)]]: (re)initialise using opened STAR file [[filestream]].
\item [[NewBlock(blockname,[block contents],replace=False)]]: add new block to this object.  If [[blockcontents]] is provided, it
must be a [[StarBlock]] object (see below).  If replace is [[False]], attempts to
replace a pre-existing block will cause an error.
\item [[WriteOut(comment)]]: return the contents of the current file as a
CIF-conformant string, with optional [[comment]] at the beginning.
\item [[set_uri(URI)]]: set the URI of the file.  Typically this is done internally when
loading from a file, or else is set to the current working directory
\end{enumerate}

The methods available for the StarBlock type are:
\begin{enumerate}
\item [[GetLoopItem(itemname)]]: return the value of [[itemname]] in 
current block (equivalent to using [])
\item [[AddLoopItem(data)]]: add [[data]] to the current block.  [[data]] is 
a tuple consisting of a single itemname and an array of 
data, or else a single data value.  This method 
is called when setting data using [].
\item [[RemoveLoopItem(dataname)]]: remove the given dataname from the 
current block.  Same as typing [['del block[item]']]
\item [[GetLoop(dataname)]]: for looped data item [[dataname]], get a list
of all itemnames and values co-occurring in this loop.  Returns an error
if [[dataname]] is not in a loop.
\item [[AddToLoop(dataname,data)]]: add [[data]] to the loop containing
[[dataname]].  If dataname is not a looped item, an error is returned.
If (data) has the wrong length, an error is returned. 
\item [[loops()]]: return a list containing all looped names, grouped into
individual loops.  This was added to facilitate validity checking and is
unlikely to be useful otherwise.
\item [[first_block()]]: returns the first block in the file.
\end{enumerate}

Note also that a StarFile object can be accessed as a mapping type, ie using
square brackets.  Most mapping operations have been implemented (see below).

We import type objects at the module level, as required by later versions
of Python.

<<*>>=
<<Copyright statement>>
from types import *
from urllib import *         # for arbitrary opening
import re
import copy
# For Python 2.6 or higher compatibility
try: 
    set
except NameError:
    import sets
    set = sets.Set

<<Define a collection datatype>>
<<LoopBlock class>>
<<StarBlock class>>
<<Star packet class>>
<<BlockCollection class>>
<<StarFile class>>
<<Subclass StringIO>>
<<Define an error class>>
<<Read in a STAR file>>
<<Get data dimension>>
<<Utility functions>>

@ \section{BlockCollection}

Starfiles and Ciffiles are both collections of blocks. We abstract
this into the [[BlockCollection]] class, and then inherit from it to
make a [[StarFile]] object.  The philosophy is that the treatment of
the constituent blocks is managed by the enclosing block collection
based on how the block collection was initialised.

<<BlockCollection class>>=
class BlockCollection(object):
    <<Initialise BC data structures>>
    <<Block collection locking>>
    <<BC emulation of mapping type>>
    <<Add a new data section>>
    <<Re-identify a data block>>
    <<Merge with another block collection>>
    <<Conformance checks>>
    <<Collect all values of a single key in all blocks>>
    <<Switch save frame scoping rules>>
    <<Parent child utilities>>
<<Write out to string representation>>

@ With the advent of CIF2, the allowed character set has expanded to
encompass most of Unicode.  Our object needs to know about this
different characterset in order to check incoming values and datanames for
conformance.  This is done via the 'characterset' keyword.

DDLm dictionaries assume that all definitions in nested save frames
are equally accessible from other nested save frames, whereas in
instance files save frames are logically insulated from other save
frames at the same or lower levels.  Block names may be duplicated if
they are in different enclosing frames, although all save frame names
have to be unique within a DDLm dictionary (as importation is allowed
to refer to the save frame names with no qualifications).  We deal
with potential duplication by appending a '+' to the access key of
legitimate save frames with duplicate names.  Our [[child_table]]
dictionary links the internal block key to its parent and mixed-case
name used when outputting the block.

If scoping is 'instance', nested datablocks are invisible and only accessible
through the 'saves' attribute, which produces a view onto the same block 
collection.

To take account of dictionaries with 10s of thousands of entries
(e.g. the PDB) we optimise block merging for speed.  Most of the
information in separate structures below could be derived from
[[child_table]], but we take the space hit for speed. The canonical
reference to a block is the lowercase version of the name. We use
these addresses to index into a table that contains the actual block
name and the parent blockname. 

<<Initialise BC data structures>>=
def __init__(self,datasource=None,standard='CIF',
             characterset='ascii',scoping='instance',parent_id=None,**kwargs):
    import collections
    self.dictionary = {}
    self.standard = standard
    self.lower_keys = set()           # short_cuts
    self.renamed = {}
    self.characterset = characterset
    self.PC = collections.namedtuple('PC',['block_id','parent'])
    self.child_table = {}
    self.visible_keys = []            # for efficiency
    self.parent_id = parent_id
    self.scoping = scoping  #will trigger setting of child table
    if isinstance(datasource,BlockCollection):
        self.merge_fast(datasource)
        self.scoping = scoping   #reset visibility
    elif isinstance(datasource,DictType):
        for key,value in datasource.items():
             self[key]= value
    self.header_comment = ''
 
@ Unlocking.  When editing dictionaries with many datablocks, we would
rather just unlock all datablocks at once.

<<Block collection locking>>=
def unlock(self):
    """Allow overwriting of all blocks in this collection"""
    for a in self.lower_keys:
        self[a].overwrite=True

def lock(self):
    """Disallow overwriting for all blocks in this collection"""
    for a in self.lower_keys:
        self[a].overwrite = False

@ Checking block name lengths.  This is not needed for a STAR block, but
is useful for CIF.

<<Check block name lengths>>=
def checklengths(self,maxlength):
    toolong = [a.block_id for a in self.child_table.values() if len(a.block_id)>maxlength]
    if toolong:
        errorstring = ""
        for bn in toolong:
            errorstring += "\n" + bn
        raise StarError( 'Following block name(s) too long: \n' + errorstring)

@ Switch scoping.  In some cases we want to hide save frames from our accesses,
in other cases we wish to make all frames visible.  Setting the scoping attribute
allows this to be swapped around.  As we could be a 'view' onto a subset of save
frames, we use the parent_id to choose the appropriate selection of datablocks.

We do not assume that no change means we don't have to do anything.

<<Switch save frame scoping rules>>=
def __setattr__(self,attr_name,newval):
    if attr_name == 'scoping':
        if newval not in ('dictionary','instance'):
            raise StarError("Star file may only have 'dictionary' or 'instance' scoping, not %s" % newval)
        if newval == 'dictionary':
            self.visible_keys = [a for a in self.lower_keys] 
        else:
            #only top-level datablocks visible
            self.visible_keys = [a[0] for a in self.child_table.items() if a[1].parent==None]
    object.__setattr__(self,attr_name,newval)

@ Emulation of a mapping type.  We also put odd little useful utilities
in this section.

<<BC emulation of mapping type>>=
def __str__(self):
    return self.WriteOut()

def __setitem__(self,key,value):
    self.NewBlock(key,value,parent=None)

def __getitem__(self,key):
    if isinstance(key,StringTypes):
       lowerkey = key.lower()
       if lowerkey in self.lower_keys:
           return self.dictionary[lowerkey]
       print 'Visible keys:' + `self.visible_keys`
       print 'All keys' + `self.lower_keys`
       print 'Child table' + `self.child_table`
       raise KeyError,'No such item %s' % key

# we have to get an ordered list of the current keys,
# as we'll have to delete one of them anyway.
# Deletion will delete any key regardless of visibility

def __delitem__(self,key):
    dummy = self[key]   #raise error if not present
    lowerkey = key.lower()
    # get rid of all children recursively as well
    children = [a[0] for a in self.child_table.items() if a[1].parent == lowerkey]
    for child in children:
        del self[child]   #recursive call
    del self.dictionary[lowerkey]
    del self.child_table[lowerkey]
    try:
        self.visible_keys.remove(lowerkey)
    except KeyError:
        pass
    self.lower_keys.remove(lowerkey)

def __len__(self):
    return len(self.visible_keys)

def __contains__(self,item):
    """Support the 'in' operator"""
    return self.has_key(item)

# We iterate over all visible
def __iter__(self):
    for one_block in self.keys():
        yield self[one_block]

# TODO: handle different case
def keys(self):
    return self.visible_keys

# changes to take case independence into account
def has_key(self,key):
    if not isinstance(key,StringTypes): return 0
    if key.lower() in self.visible_keys:
        return 1
    return 0

def get(self,key,default=None):
    if self.has_key(key):     # take account of case
        return self.__getitem__(key)
    else:
        return default

def clear(self):
    self.dictionary.clear()
    self.lower_keys = set()
    self.child_table = {}
    self.visible_keys = []

def copy(self):   
    newcopy = self.dictionary.copy()  #all blocks
    newcopy = BlockCollection('',newcopy,parent_id=self.parent_id)
    newcopy.child_table = self.child_table.copy()
    newcopy.lower_keys = self.lower_keys
    newcopy.characterset = self.characterset
    newcopy.scoping = self.scoping  #this sets visible keys
    return newcopy

def update(self,adict):
    for key in adict.keys():
        self[key] = adict[key]

def items(self):
    return [(a,self[a]) for a in self.keys()]

def first_block(self):
    """Return the 'first' block.  This is not necessarily the first block in the file."""
    if self.keys():
        return self[self.keys()[0]]

@ Parent-child utilities.  As we are now emulating parent-child relationships
using self.child_table, we provide some useful methods.

<<Parent child utilities>>=
def get_parent(self,blockname):
    """Return the name of the block enclosing [[blockname]] in canonical form (lower case)"""
    possibles = (a for a in self.child_table.items() if a[0] == blockname.lower())
    try:
        first = possibles.next()   #get first one
    except:
        raise StarError('no parent for %s' % blockname)
    try:
       second = possibles.next()
    except StopIteration:
       return first[1].parent
    raise StarError('More than one parent for %s' % blockname)

def get_roots(self):
    """Get the top-level blocks"""
    return [a for a in self.child_table.items() if a[1].parent==None]

def get_children(self,blockname,include_parent=False,scoping='dictionary'):
    """Get all children of [[blockname]] as a block collection. If [[include_parent]] is
    True, the parent block will also be included in the block collection as the root."""
    newbc = BlockCollection()
    block_lower = blockname.lower()
    proto_child_table = [a for a in self.child_table.items() if self.is_child_of_parent(block_lower,a[1].block_id)]
    newbc.child_table = dict(proto_child_table)
    if not include_parent:
       newbc.child_table.update(dict([(a[0],self.PC(a[1].block_id,None)) for a in proto_child_table if a[1].parent == block_lower]))
    newbc.lower_keys = set([a[0] for a in proto_child_table])
    newbc.dictionary = dict((a[0],self.dictionary[a[0]]) for a in proto_child_table)
    if include_parent:
        newbc.child_table.update({block_lower:self.PC(self.child_table[block_lower].block_id,None)})
        newbc.lower_keys.add(block_lower)
        newbc.dictionary.update({block_lower:self.dictionary[block_lower]})
    newbc.scoping = scoping
    return newbc

def get_immediate_children(self,parentname):
    """Get the next level of children of the given block as a list, without nested levels"""
    child_handles = [a for a in self.child_table.items() if a[1].parent == parentname.lower()]
    return child_handles

def is_child_of_parent(self,parentname,blockname):
    """Recursively search for children of blockname, case is important for now"""
    checkname = parentname.lower()
    more_children = [a[0] for a in self.child_table.items() if a[1].parent == checkname]
    if blockname.lower() in more_children:
       return True
    else:
       for one_child in more_children:
           if self.is_child_of_parent(one_child,blockname): return True
    return False
       
def set_parent(self,parentname,childname):
    """Set the parent block"""
    # first check that both blocks exist
    if parentname.lower() not in self.lower_keys:
        raise KeyError('Parent block %s does not exist' % parentname)
    if childname.lower() not in self.lower_keys:
        raise KeyError('Child block %s does not exist' % childname)
    old_entry = self.child_table[childname.lower()]
    self.child_table[childname.lower()]=self.PC(old_entry.block_id,
           parentname.lower())
    self.scoping = self.scoping #reset visibility
        
@ Adding a new block.  A new block is just a new item in our
dictionary, so we add a new entry.  We return the new block name
in case we have changed it, so the calling routine can refer to it
later.  Also, there is a limit of 75 characters for the block name length,
which we enforce here. By setting [[fix]] to true, blocknames will have
illegal whitespace changed to underscore.

[[self.standard]] is used to enforce differences in treatments of
block names. If [[self.standard]] is set at all, blocks will not
replace a previous block with the same name.  DDLm dictionaries
are not permitted identical save frame names, but those save frame
names may be identical to the enclosing datablock.  We rename the 
access key if an identically-named save frame is introduced
anywhere in the file by appending a '+'.  These renames are stored
in the rename dictionary.  The name appearing in the output file
is not changed, only the access key.  If [[self.standard]] is 'Dic',
then we put block contents before save frames in accordance with
stylistic conventions.

Note that we must take account of upper/lower case differences being irrelevant
for STAR/CIF, but that we want to preserve the original case.

To allow for nested blocks, we can specify a parent block.  When the
file is printed, the new block will appear inside the parent block. This is
how save frames are produced.

<<Add a new data section>>=
def NewBlock(self,blockname,blockcontents=None,fix=True,parent=None):
    if blockcontents is None:
       blockcontents = StarBlock()
    if self.standard is not None:
       if self.standard == 'CIF':
          self.checknamelengths(blockcontents,maxlength=75)  #
          self.checkloopnesting(blockcontents)
    if len(blockname)>75:
             raise StarError , 'Blockname %s is longer than 75 characters' % blockname
    if fix:
        newblockname = re.sub('[  \t]','_',blockname)
    else: newblockname = blockname
    new_lowerbn = newblockname.lower()
    if new_lowerbn in self.lower_keys:
        if self.standard is not None:    #already there
           toplevelnames = [a[0] for a in self.child_table.items() if a[1].parent==None]
           if parent is None and new_lowerbn not in toplevelnames:  #can give a new key to this one
              while new_lowerbn in self.lower_keys: new_lowerbn = new_lowerbn + '+'
           elif parent is not None and new_lowerbn in toplevelnames: #can fix a different one
              replace_name = new_lowerbn            
              while replace_name in self.lower_keys: replace_name = replace_name + '+'
              self._rekey(new_lowerbn,replace_name)
              # now continue on to add in the new block
              if parent.lower() == new_lowerbn:        #the new block's requested parent just got renamed!!
                  parent = replace_name
           else:
              raise StarError( "Attempt to replace existing block " + blockname)
        else:
           del self[new_lowerbn]
    self.dictionary.update({new_lowerbn:blockcontents})
    self.lower_keys.add(new_lowerbn)
    if parent is None:
       self.child_table[new_lowerbn]=self.PC(newblockname,None)
       self.visible_keys.append(new_lowerbn)
    else:
       if parent.lower() in self.lower_keys:
          if self.scoping == 'instance':
             self.child_table[new_lowerbn]=self.PC(newblockname,parent.lower())
          else:
             self.child_table[new_lowerbn]=self.PC(newblockname,parent.lower())
             self.visible_keys.append(new_lowerbn)
       else:
           print 'Warning:Parent block %s does not exist for child %s' % (parent,newblockname)  
    return new_lowerbn  #in case calling routine wants to know

@ Renaming a block.  This is a slightly intricate operation as we have to
also make sure the original children are pointed to the new blockname.  We assume
that both oldname and newname are already lower case.  We can simply change the
key used to identify the block using [[_rekey]], or we cna change the block name
that is printed using [[rename]].  In the latter case, there must be no name collisions or the
operation will fail.

<<Re-identify a data block>>=
def _rekey(self,oldname,newname,block_id=''):
    """The block with key [[oldname]] gets [[newname]] as a new key, but the printed name
       does not change unless [[block_id]] is given.  Prefer [[rename]] for a safe version."""
    move_block = self[oldname]    #old block
    is_visible = oldname in self.visible_keys
    move_block_info = self.child_table[oldname]    #old info
    move_block_children = [a for a in self.child_table.items() if a[1].parent==oldname]
    # now rewrite the necessary bits
    self.child_table.update(dict([(a[0],self.PC(a[1].block_id,newname)) for a in move_block_children]))
    del self[oldname]   #do this after updating child table so we don't delete children
    self.dictionary.update({newname:move_block})
    self.lower_keys.add(newname)
    if block_id == '':
       self.child_table.update({newname:move_block_info})
    else:
       self.child_table.update({newname:self.PC(block_id,move_block_info.parent)})
    if is_visible: self.visible_keys += [newname]

def rename(self,oldname,newname):
    """Rename datablock from [[oldname]] to [[newname]]. Both key and printed name are changed.  No
       conformance checks are conducted."""
    realoldname = oldname.lower()
    realnewname = newname.lower()
    if realnewname in self.lower_keys:
        raise StarError,'Cannot change blockname %s to %s as %s already present' % (oldname,newname,newname)
    if realoldname not in self.lower_keys:
        raise KeyError,'Cannot find old block %s' % realoldname
    self._rekey(realoldname,realnewname,block_id=newname)
    
@ Merging.  Originally, this package envisaged Cif and STAR files as collections
of either Starblocks or Cifblocks, which differed only in their capacity to
hold save frames and nested loops.  From version 4.05, we envisage Cif and Star
files as collections of StarBlocks, neither of which hold any nested save
frames. 

This was originally implemented for dictionary merging support, which is
now deprecated with the new DDLm way of combining dictionaries.  We can't
merge [[CifDic]] objects, because the internal data structures for DDL2 and
DDL1 are different (parent-child in particular), so any merge operation
would have to first recreate the original Cif structure before proceeding.

Merging can be strict, overlay or replace.  In all cases, if the block
name is different, we simply add it in.  If it is the same, in strict
mode we flag an error, in replace mode we replace it, and in overlay
mode we actually add/replace individual data items.  The default mode
will be determined from the setting of 'standard': if no standard
has been specified, the mode is 'replace', otherwise the mode is
'strict'.

If the single_block list is non-empty, we assume that we should merge
on the block level, using the given block names as the particular
blocks to merge.  This is essentially what we have to do for DDL2
dictionaries, where all the definitions are stored in save frames inside a
single block. 

Note also the related situation where we are in 'strict' mode, and
the DDL1 dictionaries both have an "on_this_dictionary" block.  So we
have an extra keyword argument "idblock" which contains a blockname
to ignore during merging, i.e. it will remain the same
as before merging. 

The suggested overlay method involves adding to loops, rather than
replacing them completely.  Identical rows must be removed, and
any key values with identical values remaining after this have to
flag an error.  We don't read in the ddl specifications themselves,
to avoid messing around with hard-coded filenames, so we require the calling
function to provide us with this file (not yet implemented).  
 
The [[match_att]] keyword allows us to match blocks/save frames on a
particular attribute, rather than the block name itself.  This means
we can do the right thing and compare [[_name]] entries rather than
block names (the default behaviour).

Note also a problem with the overlay protocol as written up in Vol. G:
if we try matching on item.name, we will run into trouble where _item.name
is looped in DDL2-style dictionaries.  We can't match on a complete match
against all item names in the list, because we would like to be able to add
item names in overlay mode.  So we have to deduce the 'main' item name
from any parent-child information that we have using a helper function which
is passed to us.

Nested save frames are emulated through child table lookups, so we should
merge this table when merging block collections.  Unless [[parent]] is
not empty, we put all new blocks on the same level.  Otherwise, any top-level
blocks in the incoming block collection (parent is None) are given the parent
specified in [[parent]].

As for [[NewBlock]], we allow duplicate save frame names in the precise situation
where one of the blocks is a top-level block.

<<Merge with another block collection>>=
def merge_fast(self,new_bc,parent=None):
    """Do a fast merge"""
    if self.standard is None:
        mode = 'replace' 
    else:
        mode = 'strict'
    overlap_flag = not self.lower_keys.isdisjoint(new_bc.lower_keys)
    if overlap_flag and mode != 'replace':
        double_keys = self.lower_keys.intersection(new_bc.lower_keys)
        for dup_key in double_keys:
              our_parent = self.child_table[dup_key].parent
              their_parent = new_bc.child_table[dup_key].parent
              if (our_parent is None and their_parent is not None and parent is None) or\
                  parent is not None:  #rename our block
                start_key = dup_key
                while start_key in self.lower_keys: start_key = start_key+'+'
                self._rekey(dup_key,start_key)
                if parent.lower() == dup_key:  #we just renamed the prospective parent!
                    parent = start_key
              elif our_parent is not None and their_parent is None and parent is None:
                start_key = dup_key
                while start_key in new_bc.lower_keys: start_key = start_key+'+'
                new_bc._rekey(dup_key,start_key)
              else: 
                raise StarError("In strict merge mode:duplicate keys %s" % dup_key)
    self.dictionary.update(new_bc.dictionary) 
    self.lower_keys.update(new_bc.lower_keys)
    self.visible_keys += (list(new_bc.lower_keys))
    self.child_table.update(new_bc.child_table)
    if parent is not None:     #redo the child_table entries
          reparent_list = [(a[0],a[1].block_id) for a in new_bc.child_table.items() if a[1].parent==None]
          reparent_dict = [(a[0],self.PC(a[1],parent.lower())) for a in reparent_list]
          self.child_table.update(dict(reparent_dict))

def merge(self,new_bc,mode=None,parent=None,single_block=[],
               idblock="",match_att=[],match_function=None):
    if mode is None:
        if self.standard is None:
           mode = 'replace'
        else:
           mode = 'strict'
    if single_block:
        self[single_block[0]].merge(new_bc[single_block[1]],mode,
					       match_att=match_att,
                                               match_function=match_function)
        return None
    base_keys = [a[1].block_id for a in self.child_table.items()]
    block_to_item = base_keys   #default
    new_keys = [a[1].block_id for a in new_bc.child_table.items()]    #get list of incoming blocks
    if match_att:
        #make a blockname -> item name map
        if match_function:
            block_to_item = map(lambda a:match_function(self[a]),self.keys())
	else:
            block_to_item = map(lambda a:self[a].get(match_att[0],None),self.keys())
	#print `block_to_item`
    for key in new_keys:        #run over incoming blocknames
        if key == idblock: continue    #skip dictionary id
	basekey = key           #default value
        if len(match_att)>0:
	   attval = new_bc[key].get(match_att[0],0)  #0 if ignoring matching
	else:
           attval = 0
        for ii in range(len(block_to_item)):  #do this way to get looped names
	    thisatt = block_to_item[ii]       #keyname in old block
	    #print "Looking for %s in %s" % (attval,thisatt)
            if attval == thisatt or \
	       (isinstance(thisatt,ListType) and attval in thisatt):
	          basekey = base_keys.pop(ii)
                  block_to_item.remove(thisatt)
                  break
        if not self.has_key(basekey) or mode=="replace":
            new_parent = new_bc.get_parent(key)
            if parent is not None and new_parent is None:
               new_parent = parent
            self.NewBlock(basekey,new_bc[key],parent=new_parent)   #add the block
        else:
            if mode=="strict":
                raise StarError( "In strict merge mode: block %s in old and block %s in new files" % (basekey,key))
            elif mode=="overlay":
                # print "Merging block %s with %s" % (basekey,key)
                self[basekey].merge(new_bc[key],mode,match_att=match_att)
            else:  
                raise StarError( "Merge called with unknown mode %s" % mode)
     
@ Checking conformance. CIF and STAR standards differ in allowing
nested loops and maximum data name lengths.  Although the CIF 1.1
standard allows very long lines (2048 characters), data names are
still restricted to be no more than 75 characters in length in the CIF
standard.

<<Conformance checks>>=
def checknamelengths(self,target_block,maxlength=-1):
    if maxlength < 0:
        return
    else:
        toolong = filter(lambda a:len(a)>maxlength, target_block.keys())
    outstring = ""
    for it in toolong: outstring += "\n" + it
    if toolong:
       raise StarError( 'Following data names too long:' + outstring)

def checkloopnesting(self,target_block):
    """Check that block doesn't contain nested loops"""
    for one_loop in target_block.loops:
        if len(one_loop.loops) > 0:
           raise StarError('Block contains nested loops')

@ When validating DDL2-type dictionaries against the DDL spec file, we have
to be able to see all values of parent data items across all save frames
in order to validate parent-child relations (I've inferred this, but
if I ever find a standard document this may turn out to be wrong).  So this
method is provided to return a list of all values taken by the given
attribute within all of the blocks inside a block collection.

A flat list is returned, even if looped values happen to occur in a data
block.  This is because the one routine that calls this method is 
interested in whether or not a given value occurs, rather than how 
it occurs or what it occurs with.  We also remove duplicate values.

<<Collect all values of a single key in all blocks>>=
def get_all(self,item_name):
    raw_values = map(lambda a:self[a].get(item_name),self.keys())
    raw_values = filter(lambda a:a != None, raw_values)
    ret_vals = []
    for rv in raw_values:
        if isinstance(rv,ListType):
	    for rvv in rv:
		if rvv not in ret_vals: ret_vals.append(rvv)
	else:
	    if rv not in ret_vals: ret_vals.append(rv)
    return ret_vals

@ Writing all this stuff out to a string.  We loop over each of the
individual sections, getting their string representation.  We 
implement this using the cStringIO module for faster work.  Note that
the default output comment specifies a CIF 1.1 standard file.

Note that child blocks must be save frames, so we hard-code 'save'.

<<Write out to string representation>>=
    def WriteOut(self,comment='',wraplength=80,maxoutlength=2048):
        import cStringIO
        if not comment:
            comment = self.header_comment
        outstring = cStringIO.StringIO()
        outstring.write(comment)
        # loop over top-level
        top_block_names = [(a[0],a[1].block_id) for a in self.child_table.items() if a[1].parent is None]
        for blockref,blockname in top_block_names:
            outstring.write('\n' + 'data_' +blockname+'\n')
            child_names = [(a[0],a[1].block_id) for a in self.child_table.items() if a[1].parent==blockref]
            if self.standard == 'Dic':
              self[blockref].SetOutputLength(wraplength,maxoutlength)
              outstring.write(str(self[blockref]))
            for child_ref,child_name in child_names:
                outstring.write('\n' + 'save_' + child_name + '\n')
                self.block_to_string(child_ref,child_name,outstring,1)    
                outstring.write('\n' + 'save_'+ '\n')   
            if self.standard != 'Dic':
                self[blockref].SetOutputLength(wraplength,maxoutlength)
                outstring.write(str(self[blockref]))
        returnstring =  outstring.getvalue()
        outstring.close()
        return returnstring

    def block_to_string(self,block_ref,block_id,outstring,indentlevel=0):
        """Output a complete datablock indexed by [[block_ref]] and named [[block_id]], including children"""
        child_names = [(a[0],a[1].block_id) for a in self.child_table.items() if a[1].parent==block_ref]
        if self.standard == 'Dic':
            outstring.write(str(self[block_ref]))
        for child_ref,child_name in child_names:
            outstring.write('\n' + '  '*indentlevel + 'save_' + child_name + '\n')
            self.block_to_string(child_ref,child_name,outstring,indentlevel+1)
            outstring.write('\n' + '  '*indentlevel + 'save_' + '\n')
        if self.standard != 'Dic':
            outstring.write(str(self[block_ref]))
        
@ \section{StarFile}

If we are passed a filename, we open it and read it in, assuming that
it is a conformant STAR file.  A StarFile object is a dictionary of
StarBlock objects, accessed by block name. 
Parameter [[maxoutlength]] sets the maximum line size for output.  If
[[maxoutlength]] is not specified, it defaults to the maximum input
length.

<<StarFile class>>=
class StarFile(BlockCollection):
<<Initialise data structures>>
<<Set URI>>

@ When initialising, we add those parts that are unique to the StarFile as
opposed to a simple collection of blocks - i.e. reading in from a file,
and some line length restrictions.  We don't indent this section in the
noweb file, so that our comment characters output at the beginning of the
line.  

<<Initialise data structures>>=
    def __init__(self,datasource=None,maxinlength=-1,maxoutlength=0,
                scoping='instance',grammar='1.1',scantype='standard',**kwargs):
        super(StarFile,self).__init__(datasource=datasource,**kwargs)
        self.my_uri = getattr(datasource,'my_uri','')
        self.maxinlength = maxinlength      #no restriction
        if maxoutlength == 0:
            self.maxoutlength = 2048 
        else:
            self.maxoutlength = maxoutlength
        self.scoping = scoping
        if type(datasource) in StringTypes or hasattr(datasource,"read"):
            ReadStar(datasource,prepared=self,maxlength=self.maxinlength,
                        grammar=grammar,scantype=scantype)
        self.header_comment = \
"""#\\#STAR
##########################################################################
#               STAR Format file 
#               Produced by PySTARRW module
# 
#  This is a STAR file.  STAR is a superset of the CIF file type.  For
#  more information, please refer to International Tables for Crystallography,
#  Volume G, Chapter 2.1
#
##########################################################################
"""
@ A function to make sure we have the correct file location

<<Set URI>>=
    def set_uri(self,my_uri): self.my_uri = my_uri

@ Reading in a file.  We now use the Yapps2-generated [[YappsStarParser]] module to provide grammar
services.  The structure returned from parsing is a StarFile, with
possible grammar violations due to duplicate block or item names.

We allow fast reads using the compiled StarScan module
by passing the option 'flex' to this routine.  We also permit an
already-opened stream to be passed to us (thanks to Boris Dusek for
this contribution).  There are 3 possible syntax variations: very old
CIF files allowed unquoted data values to begin with open brackets,
version 1.1 disallowed this, and DDLm-conformant files interpret these
as actual bracket expressions.  The different grammars are
selected by a command-line switch.  

We allow reading CBF files, which can contain binary sections, by
removing all characters found between the strings '-BINARY-FORMAT-SECTION'.
This is not a robust approach as this string could theoretically be found
in a comment or datavalue.

We save our URL for possible later use in finding files relative to the location of
this file e.g. with DDLm dictionary imports.

<<Read in a STAR file>>=
def ReadStar(filename,prepared = StarFile(),maxlength=2048,scantype='standard',grammar='1.1',CBF=False):
    import string
    import codecs
    # save desired scoping
    save_scoping = prepared.scoping
    if grammar=="1.1":
        import YappsStarParser_1_1 as Y
    elif grammar=="1.0":
        import YappsStarParser_1_0 as Y
    elif grammar=="DDLm":
        import YappsStarParser_DDLm as Y
    if isinstance(filename,basestring):
        filestream = urlopen(filename)
    else:
	filestream = filename   #already opened for us
    my_uri = ""
    if hasattr(filestream,"geturl"): 
        my_uri = filestream.geturl()
    text = unicode(filestream.read(),"utf8")
    if isinstance(filename,basestring): #we opened it, we close it
        filestream.close()
    if not text:      # empty file, return empty block
        return StarFile().set_uri(my_uri)
    # filter out non-ASCII characters in CBF files if required.  We assume
    # that the binary is enclosed in a fixed string that occurs
    # nowhere else.
    if CBF:
       text_bits  = text.split("-BINARY-FORMAT-SECTION-") 
       text = text_bits[0] 
       for section in range(2,len(text_bits),2):
           text = text+" (binary omitted)"+text_bits[section]
    # we recognise ctrl-Z as end of file
    endoffile = text.find('\x1a')
    if endoffile >= 0: 
        text = text[:endoffile]
    split = string.split(text,'\n')
    if maxlength > 0:
        toolong = filter(lambda a:len(a)>maxlength,split)
        if toolong:
            pos = split.index(toolong[0])
            raise StarError( 'Line %d contains more than %d characters' % (pos+1,maxlength))
    if scantype == 'standard':
            parser = Y.StarParser(Y.StarParserScanner(text))
    else:
	    parser = Y.StarParser(Y.yappsrt.Scanner(None,[],text,scantype='flex'))
    proto_star = None
    try:
        proto_star = getattr(parser,"input")(prepared)
    except Y.yappsrt.SyntaxError,e:
           input = parser._scanner.input
           Y.yappsrt.print_error(input, e, parser._scanner)
    except Y.yappsrt.NoMoreTokens:
           print >>sys.stderr, 'Could not complete parsing; stopped around here:'
           print >>sys.stderr, parser._scanner
    if proto_star == None:
        errorstring = 'Syntax error in input file: last value parsed was %s' % Y.lastval
        errorstring = errorstring + '\nParser status: %s' % `parser._scanner`
        raise StarError( errorstring)
    # set visibility correctly
    proto_star.scoping = 'dictionary'
    # duplication check on all blocks
    audit_result = map(lambda a:(a,proto_star[a].audit()),proto_star.keys())
    audit_result = filter(lambda a:len(a[1])>0,audit_result)
    if audit_result:
        raise StarError( 'Duplicate keys as follows: %s' % `audit_result`)
    proto_star.set_uri(my_uri)
    proto_star.scoping = save_scoping
    return proto_star

@ \section{Collection datatype}

DDLm introduced data values which could be lists, tuples or hash tables.  As
we use lists in the API for loop values, and think this is convenient, we
define a pseudo collection class to hold CIF lists/tuples/hashes.  Note that
we have to recursively modify all contents as well.  We also need to define
custom output functions in order to properly format the internal datatypes.

<<Define a collection datatype>>=
class StarList(list):
    pass

class StarDict(dict):
    pass


@ \section{Loop Block class}

This is the fundamental building block of a StarFile.  We abstract a
loop to mean a collection of tag value pairs and a collection of 
zero or more loop blocks (recursive definition).  The values have a
dimension that increases by one with each embedding. In practice we
expect only a single level of embedding, i.e. dimension 1 or 0, but
due to the historical STAR standard allowing multiple levels of loop
block nesting we can handle more.

A CifBlock inherits from a CifLoopBlock which inherits from a Star
Block which inherits from this LoopBlock class.
 
We store the dimension in the class for convenience.

A Star Block is simply a Loop block with dimension zero.

<<LoopBlock class>>=
class LoopBlock(object):
    <<Initialise Loop Block>>
    <<Add emulation of a mapping type>>
    <<Selection of iterators>>
    <<Insert a nested loop>>
    <<Remove a nested loop>>
    <<Add a comment>>
    <<Remove a comment>>
    <<Return value of item>>
    <<Remove a data item>>
    <<Add a data item>>
    <<Check data name for STAR conformance>>
    <<Check data item for STAR conformance>>
    <<Regularise data values>>
    <<Get complete looped data>>
    <<Get nth loop packet>>
    <<Add a packet>>
    <<Remove a packet>>
    <<Get packet by key>>
    <<Get item order>>
    <<Change data item order>>
    <<Return position of data item>>
    <<Collapse to nth packet>>
    <<Audit for repeated names>>
    <<Get co-looped names>>
    <<Add to looped data>>
    <<Transform a group of tags to a subgroup>>
    <<Functions for printing out>>

@ If given non-zero data to initialise the block with, we either
copy (if it is a dictionary) or else initialise each key-value
pair separately (if tuples).  We take care to include our special
"loop" key if it is not in the supplied dictionary, but apart from
this we make no check of the actual conformance of the dictionary
items. 

The dimension parameter refers to the number of dimenstions of the
value; zero would be a single value, 1 is a 1-dimensional array, etc.

To manage case insensitivity while preserving the case of items
that we are passed, we store a list of lower-case keys so that we
are not constantly calling the [[lower()]] method of the strings.  This
list applies only to the items in the body of the loop, not to any
items in nested loops.  However, when searching for items and returning
items, nested loops are searched.

The [[overwrite]] argument allows values to be silently replaced, as per a
normal python dictionary.  However, when reading in from a file, we want to
detect duplicated values, so we set this to false.  As DDLm introduces the 
unicode character set, we need to indicate which character set we are
prepared to accept.

<<Initialise Loop Block>>=
def __init__(self,data = (), dimension = 0, maxoutlength=2048, wraplength=80, overwrite=True,
             characterset='ascii'):
    # print 'Creating new loop block, dimension %d' % dimension
    self.block = {}
    self.loops = []
    self.no_packets = 0
    self.item_order = []
    self.lower_keys = []    #for efficiency
    self.comment_list = {}
    self.dimension = dimension
    self.popout = False         #used during load iteration
    self.curitem = -1           #used during iteration
    self.maxoutlength = maxoutlength
    self.wraplength = wraplength
    self.overwrite = overwrite
    self.characterset = characterset
    if not hasattr(self,'loopclass'):  #in case are derived class
        self.loopclass = LoopBlock  #when making new loops
    if self.characterset == 'ascii':
        self.char_check = re.compile("[][ \n\r\t!%&\(\)*+,./:<=>?@0-9A-Za-z\\\\^`{}\|~\"#$';_-]+",re.M)
    elif self.characterset == 'unicode':
        self.char_check = re.compile(u"[][ \n\r\t!%&\(\)*+,./:<=>?@0-9A-Za-z\\\\^`{}\|~\"#$';_\u00A0-\uD7FF\uE000-\uFDCF\uFDF0-\uFFFD\U00010000-\U0010FFFD-]+",re.M)
    else:
        raise StarError("No character set specified")
    if isinstance(data,(TupleType,ListType)):
        for item in data:
            self.AddLoopItem(item)
    elif isinstance(data,LoopBlock):
        self.block = data.block.copy() 
        self.item_order = data.item_order[:]
        self.lower_keys = data.lower_keys[:]
	self.comment_list = data.comment_list.copy()
        self.dimension = data.dimension
        # loops as well
        for loopno in range(len(data.loops)):
            try:
                placeholder = self.item_order.index(data.loops[loopno])
            except ValueError:
                print "Warning: loop %s (%s) in loops, but not in item_order (%s)" % (`data.loops[loopno]`,str(data.loops[loopno]),`self.item_order`)
                placeholder = -1
            self.item_order.remove(data.loops[loopno])   #gone
            # newobject = self.loopclass(data.loops[loopno])
            # print "Recasting and adding loop %s -> %s" % (`data.loops[loopno]`,`newobject`)
            self.insert_loop(data.loops[loopno],position=placeholder)

@ Adding emulation of a mapping type.  We add any of the other
functions we'd like to emulate.   [[__len__]] returns the number
of items in this block, either in a loop or not.  So it is
not the simple length of the dictionary.

<<Add emulation of a mapping type>>=
def __str__(self):
    return self.printsection()

def __setitem__(self,key,value):
    self.AddLoopItem((key,value))

def __getitem__(self,key):
    if isinstance(key,IntType):   #return a packet!!
        return self.GetPacket(key)        
    return self.GetLoopItem(key)

def __delitem__(self,key):
    self.RemoveLoopItem(key)

def __len__(self):
    blen = len(self.block)
    for aloop in self.loops:
        # print 'Aloop is %s' % `aloop`
        blen = blen + len(aloop)  # also a LoopBlock
    return blen    

def __nonzero__(self):
    if self.__len__() > 0: return 1
    return 0

# keys returns all internal keys
def keys(self):
    thesekeys = self.block.keys()
    for aloop in self.loops:
        thesekeys.extend(aloop.keys())
    return thesekeys

def values(self):
    ourkeys = self.keys()
    return map(lambda a:self[a],ourkeys)

def items(self):
    ourkeys = self.keys()
    return map(lambda a,b:(a,b),self.keys(),self.values())

def has_key(self,key):
    if isinstance(key,StringTypes) and key.lower() in self.lower_keys:
        return 1
    for aloop in self.loops:
        if aloop.has_key(key): return 1
    return 0

def get(self,key,default=None):
    if self.has_key(key):
        retval = self.GetLoopItem(key)
    else:
        retval = default
    return retval

def clear(self):
    self.block = {}
    self.loops = []
    self.item_order = []
    self.lower_keys = []
    self.no_packets = 0

# doesn't appear to work
def copy(self):
    newcopy = LoopBlock(dimension = self.dimension)
    newcopy.block = self.block.copy()
    newcopy.loops = []
    newcopy.no_packets = self.no_packets
    newcopy.item_order = self.item_order[:]
    newcopy.lower_keys = self.lower_keys[:]
    for loop in self.loops:
        try:
            placeholder = self.item_order.index(loop)
        except ValueError:
            print "Warning: loop %s (%s) in loops, but not in item_order (%s)" % (`loop`,str(loop),`self.item_order`)
            placeholder = -1
        newcopy.item_order.remove(loop)   #gone
        newobject = loop.copy()
        # print "Adding loop %s -> %s" % (`loop`,`newobject`)
        newcopy.insert_loop(newobject,position=placeholder)
    return newcopy

# this is not appropriate for subloops.  Instead, the loop block
# should be accessed directly for update
 
def update(self,adict):
    for key in adict.keys():
        self.AddLoopItem((key,adict[key]))

@ There are two potential ways of running over the data in a LoopBlock: we
could loop over the set of values in the non-nested values, and return the
corresponding nested loop packets in a LoopBlock (a one level iterator),
in which case the calling program decides whether or not it wants to
dig deeper; or we could recursively expand and loop over all nested loops
as well.  We set the default behaviour on initialisation to be one-level.
 
<<Selection of iterators>>=
<<A load iterator>>
<<A recursive iterator>>
<<A one-level iterator>>

@ When loading values, we want to iterate over the items until
a "stop_" token is found - this is communicated via the "popout"
attribute changing to True.  We save the __iter__ method for
iterating over packets.  Also, when a new packet is begun, all
subloops should be extended correspondingly.  We are in a special
situation where we don't enforce length matching, as we assume
that things will be loaded in as we go. 

Each yield returns a list which should be appended to with a unitary
item.  So, as the number of packets increases, we need to make sure
that the lowest level lists are extended as needed with empty lists.

<<A load iterator>>=
def load_iter(self,coords=[]):
    count = 0        #to create packet index 
    while not self.popout:
        # ok, we have a new packet:  append a list to our subloops
        for aloop in self.loops:
            aloop.new_enclosing_packet()
        for iname in self.item_order:
            if isinstance(iname,LoopBlock):       #into a nested loop
	        for subitems in iname.load_iter(coords=coords+[count]):
                    # print 'Yielding %s' % `subitems`
	            yield subitems
                # print 'End of internal loop'
            else:
                if self.dimension == 0:
                    # print 'Yielding %s' % `self[iname]`
      	            yield self,self[iname]
                else:
                    backval = self.block[iname]
                    for i in range(len(coords)):
                       # print 'backval, coords: %s, %s' % (`backval`,`coords`)
                       backval = backval[coords[i]]
                    yield self,backval
        count = count + 1      # count packets
    self.popout = False        # reinitialise
    # print 'Finished iterating'
    yield self,'###Blank###'     #this value should never be used

# an experimental fast iterator for level-1 loops (ie CIF)
def fast_load_iter(self):
    targets = map(lambda a:self.block[a],self.item_order)
    while targets:
        for target in targets:
            yield self,target

# Add another list of the required shape to take into account a new outer packet
def new_enclosing_packet(self):
    if self.dimension > 1:      #otherwise have a top-level list
        for iname in self.keys():  #includes lower levels
            target_list = self[iname]
            for i in range(3,self.dimension): #dim 2 upwards are lists of lists of... 
                target_list = target_list[-1]
            target_list.append([])
            # print '%s now %s' % (iname,`self[iname]`)

@ We recursively expand out all values in nested loops and return a 
simple dictionary type.  Although it only seems to make sense to call
this from a dimension 0 LoopBlock, if we are not a level 0 LoopBlock, 
we drill down until we get a simple value to return, then start looping.

We want to build up a return dictionary by adding keys from the deeper loops,
but if we simply use the dictionary update method, we will find that we
have stale keys from previous inner loops.  Therefore, we keep our values as
(key,value) tuples which we turn into a Star packet at the last moment.

This is now updated to return StarPackets, which are like lists
except that they also have attributes set.
 
<<A recursive iterator>>=
def recursive_iter(self,dict_so_far={},coord=[]):
    # print "Recursive iter: coord %s, keys %s, dim %d" % (`coord`,`self.block.keys()`,self.dimension)
    my_length = 0
    top_items = self.block.items()
    top_values = self.block.values()       #same order as items
    drill_values = self.block.values()
    for dimup in range(0,self.dimension):  #look higher in the tree
        if len(drill_values)>0:            #this block has values
            drill_values=drill_values[0]   #drill in
        else:
            raise StarError("Malformed loop packet %s" % `top_items[0]`)
    my_length = len(drill_values)
    if self.dimension == 0:                #top level
        for aloop in self.loops:
            for apacket in aloop.recursive_iter():
                # print "Recursive yielding %s" % `dict(top_items + apacket.items())`
                prep_yield = StarPacket(top_values+apacket.values())  #straight list
                for name,value in top_items + apacket.items():
                    setattr(prep_yield,name,value)
                yield prep_yield
    else:                                  #in some loop
        for i in range(my_length):
            kvpairs = map(lambda a:(a,self.coord_to_group(a,coord)[i]),self.block.keys())
            kvvals = map(lambda a:a[1],kvpairs)   #just values
            # print "Recursive kvpairs at %d: %s" % (i,`kvpairs`)
            if self.loops:
              for aloop in self.loops:
                for apacket in aloop.recursive_iter(coord=coord+[i]):
                    # print "Recursive yielding %s" % `dict(kvpairs + apacket.items())`
                    prep_yield = StarPacket(kvvals+apacket.values())
                    for name,value in kvpairs + apacket.items():
                        setattr(prep_yield,name,value)
                    yield prep_yield
            else:           # we're at the bottom of the tree
                # print "Recursive yielding %s" % `dict(kvpairs)`
                prep_yield = StarPacket(kvvals)
                for name,value in kvpairs:
                    setattr(prep_yield,name,value)
                yield prep_yield

# small function to use the coordinates. 
def coord_to_group(self,dataname,coords):
      if not isinstance(dataname,StringTypes):
         return dataname     # flag inner loop processing
      newm = self[dataname]          # newm must be a list or tuple
      for c in coords:
          # print "Coord_to_group: %s ->" % (`newm`),
          newm = newm[c]
          # print `newm`
      return newm 

@ Return a series of LoopBlocks with the appropriate packet chosen. This
does not loop over interior blocks, so called at the top level it just
returns the whole star block.

<<A one-level iterator>>=
def flat_iterator(self):
    if self.dimension == 0:   
        yield copy.copy(self)
    else:
        my_length = 0
        top_keys = self.block.keys()
        if len(top_keys)>0:
            my_length = len(self.block[top_keys[0]])
        for pack_no in range(my_length):
            yield(self.collapse(pack_no))
        
@ Insert a subloop.  Rather than a simple append, we need to register
the order in which this loop appears, by putting its integer index
into our item_order array.  We can optionally check for duplicate
values, which is normally a good idea; however, if we are reading in 
a file, for efficiency we only do this at the end of input.

<<Insert a nested loop>>=
def insert_loop(self,newloop,position=-1,audit=True):
    # check that new loop is kosher
    if newloop.dimension != self.dimension + 1:
        raise StarError( 'Insertion of loop of wrong nesting level %d, should be %d' % (newloop.dimension, self.dimension+1))
    self.loops.append(newloop)
    if audit:
        dupes = self.audit()
        if dupes:
            dupenames = map(lambda a:a[0],dupes)
            raise StarError( 'Duplicate names: %s' % `dupenames`)
    if position >= 0:
        self.item_order.insert(position,newloop)
    else:
        self.item_order.append(newloop)
    # print "Insert loop: item_order now" + `self.item_order`

<<Remove a nested loop>>=
def remove_loop(self,oldloop):
    # print "Removing %s: item_order %s" % (`oldloop`,self.item_order)
    # print "Length %d" % len(oldloop)
    self.item_order.remove(oldloop)
    self.loops.remove(oldloop)
 
@ Dealing with comments.  Comments are attached to a data name, and
will be printed on the line before that name appears.

<<Add a comment>>=
def AddComment(self,itemname,comment):
    self.comment_list[itemname.lower()] = comment

<<Remove a comment>>=
def RemoveComment(self,itemname):
    del self.comment_list[itemname.lower()]

@ Returning an item value.  Note that a looped block has little
meaning without all the items in the loop.  Routine [[GetLoop]] is
better in this case.  This is a real time-intensive loop, so we
initially assume that the key we have been passed is the right
key (i.e. case is the same) and only check for case if this
fails.

<<Return value of item>>=
def GetLoopItem(self,itemname):
    # assume case is correct first
    try:
	return self.block[itemname]
    except KeyError:
	for loop in self.loops:
	    try:
		return loop[itemname]
	    except KeyError:
		pass
    if itemname.lower() not in self.lower_keys:
        raise KeyError, 'Item %s not in block' % itemname
    # it is there somewhere, now we need to find it
    real_keys = self.block.keys()
    lower_keys = map(lambda a:a.lower(),self.block.keys()) 
    try:
	k_index = lower_keys.index(itemname.lower())
    except ValueError:
        raise KeyError, 'Item %s not in block' % itemname
    return self.block[real_keys[k_index]]

@ This function returns the particular loop block containing the
specified dataname, so that we can manipulate its contents directly.

<<Get complete looped data>>=
def GetLoop(self,keyname):
    if keyname in self.block:        #python 2.2 or above
        return self
    for aloop in self.loops:
        try: 
            return aloop.GetLoop(keyname)
        except KeyError:
            pass
    raise KeyError, 'Item %s does not exist' % keyname

@ Get nth looped packet.  This returns a packet of data, including any
nested loops.  For a nested loop, we want the set of packets corresponding
to the nth outer packet; so after picking out the appropriate elements, we
have to transpose so that we have a packet.

We return a StarPacket object, which looks very much like a list, in order
to support the proposed DDLm semantics of allowing a particular value to
be accessed by attribute.

<<Star packet class>>=
class StarPacket(list):
    pass

<<Get nth loop packet>>=
def GetPacket(self,index):
    thispack = StarPacket([])
    for myitem in self.item_order:
        if isinstance(myitem,LoopBlock):
            pack_list = map(lambda b:myitem[b][index],myitem.item_order)
            # print 'Pack_list -> %s' % `pack_list`
            thispack.append(pack_list)
        elif self.dimension==0:
            thispack.append(self[myitem])
        else:
            thispack.append(self[myitem][index])
            setattr(thispack,myitem,thispack[-1])
    return thispack 

@ Adding a packet.  We are passed a StarPacket object, which is just a list which is accessible by attribute.
As I have not yet produced a proper __init__ or __new__ method to allow creation of a new StarPacket, it is
advisable to create a new packet by copying an old packet.  This has not been written for nested loops, but
only for a single-level Cif-style loop. 

<<Add a packet>>=
def AddPacket(self,packet):
    if self.dimension==0:
        raise StarError,"Attempt to add packet to top level block"
    for myitem in self.item_order:
        self[myitem] = list(self[myitem])   #in case we have stored a tuple
        self[myitem].append(packet.__getattribute__(myitem))
    self.no_packets +=1 
        # print "%s now %s" % (myitem,`self[myitem]`)
    
@ The draft DDLm specification uses square brackets next to a pre-specified
identifier to mean "the packet containing this key item".  As the meaning in
PyCIFRW is always that of a simple table, we instead implement a function which
fullfils this role and pre-process the DRel script later to remove the
square brackets where necessary.   At the LoopBlock level we have no idea as
to which data name is the key, so that is passed to us from the dictionary
processing layer.  Note we assume a single key rather than multiple keys for
this call, and let the calling layer handle multiple or missing packets.

We guarantee to return a single packet, even if multiple packets match.
Perhaps we should raise an error in this case.

<<Get packet by key>>=
def GetKeyedPacket(self,keyname,keyvalue):
    """Return the loop packet where [[keyname]] has value [[keyvalue]]"""
    #print "Looking for %s in %s" % (keyvalue, self[keyname])
    my_loop = self.GetLoop(keyname)
    one_pack= [a for a in my_loop if getattr(a,keyname)==keyvalue]
    if len(one_pack)!=1:
        raise KeyError, "Bad packet key %s = %s: returned %d packets" % (keyname,keyvalue,len(one_pack))
    #print "Keyed packet: %s" % one_pack[0]
    return one_pack[0]

@ We might also want to remove a packet by key.  We operate on the data
in place.

<<Remove a packet>>=
def RemoveKeyedPacket(self,keyname,keyvalue):
    packet_coord = list(self[keyname]).index(keyvalue)
    loophandle = self.GetLoop(keyname)
    for packet_entry in loophandle.item_order:
        loophandle[packet_entry] = list(loophandle[packet_entry])
        del loophandle[packet_entry][packet_coord]
    self.no_packets -= 1
    
@ Return order of items - this is just a copy of our item_order array.

<<Get item order>>=
def GetItemOrder(self):
    return self.item_order[:]

@ Move an item to a different position in the loop.  This only affects
the printout order.  We allow different capitalisation and have to
absorb the possibility of nested loops in the order list, and being
passed a loop reference in the [[itemname]] argument.  

<<Change data item order>>=
def ChangeItemOrder(self,itemname,newpos):
    testpos = self.GetItemPosition(itemname)
    del self.item_order[testpos]
    # so we have an object ready for action
    self.item_order.insert(newpos,itemname)

@ A utility function to get the numerical order in the printout
of the given item.  We have the try: except: clauses in there as
we may have Loop or Comment classes in our order list.

<<Return position of data item>>=
def GetItemPosition(self,itemname):
    import string
    def low_case(item):
        try:
            return string.lower(item)
        except AttributeError:
            return item
    try:
        testname = string.lower(itemname)
    except AttributeError: 
        testname = itemname
    lowcase_order = map(low_case,self.item_order)
    return lowcase_order.index(testname)

@ This returns a copy, in theory independent (check that) with just
the nth packet selected, and order preserved.

<<Collapse to nth packet>>=
def collapse(self,packet_no):
    if self.dimension == 0:
        raise StarError( "Attempt to select non-existent packet")
    newlb = LoopBlock(dimension=self.dimension-1)
    for one_item in self.item_order:
        if isinstance(one_item,LoopBlock):
            newlb.insert_loop(one_item.collapse(packet_no))
        else:
            # print "Collapse: %s -> %s" % (one_item,`self[one_item][packet_no]`)
            newlb[one_item] = self[one_item][packet_no] 
    return newlb
    
@ This function is typically called once by the topmost loop after reading
in a complete datablock; if it returns an empty list, that is a guarantee
that no datanames are repeated within this loop and subloops.  We use the
sets module for efficiency (when we go to 2.4 support we'll use the builtin
as well).

<<Audit for repeated names>>=
def audit(self):
    allkeys = self.keys()
    uniquenames = set(allkeys)
    if len(uniquenames) == len(allkeys): return []
    else:              
        keycount = map(lambda a:(a,allkeys.count(a)),uniquenames)
        return filter(lambda a:a[1]>1,keycount)
    
@ Get co-looped names.  Sometimes we just want names, and will get the
values ourselves on a need-to-know basis. 

<<Get co-looped names>>=
def GetLoopNames(self,keyname):
    if keyname in self:
        return self.keys()
    for aloop in self.loops:
        try: 
            return aloop.GetLoopNames(keyname)
        except KeyError:
            pass
    raise KeyError, 'Item does not exist'

@ Adding to a loop.  We find the loop containing the dataname that
we've been passed, and then append all of the (key,values) pairs that we
are passed in [[data]], which is a dictionary.  We expect that the data 
have been sorted out for us, unlike when data are passed in [[AddLoopItem]], 
when there can be both unlooped and looped data in one set.  The dataname 
passed to this routine is simply a convenient way to refer to the
loop, and has no other significance.

<<Add to looped data>>=
def AddToLoop(self,dataname,loopdata):
    thisloop = self.GetLoop(dataname)
    for itemname,itemvalue in loopdata.items():
        thisloop[itemname] = itemvalue

@ A single-valued set of tags at the top level is isomorphic to those tags
placed in a loop with a single packet.  Likewise, looped data can be
transferred to a star sub-loop.

<<Transform a group of tags to a subgroup>>=
def Loopify(self,datanamelist):
    thisloop = self.GetLoop(datanamelist[0])
    badmatch = filter(lambda a:a in datanamelist,thisloop.keys())
    if len(badmatch)==len(datanamelist):    #all at same level so is OK
        newloop = LoopBlock(dimension=self.dimension+1)
        for name in datanamelist: 
            newloop[name]=[self[name]]
            del self[name]
        self.insert_loop(newloop) 
 
@ Removing a data item.  We delete the item, and if it is looped, and
nothing is left in the loop, we remove that element of the list. 

<<Remove a data item>>=
def RemoveLoopItem(self,itemname):
    if self.has_key(itemname):
        testkey = itemname.lower()
        real_keys = self.block.keys()
        lower_keys = map(lambda a:a.lower(),real_keys)
        try:
            k_index = lower_keys.index(testkey)
        except ValueError:    #must be in a lower loop
            for aloop in self.loops:
                if aloop.has_key(itemname):
                    # print "Deleting %s (%s)" % (itemname,aloop[itemname])
                    del aloop[itemname]
                    if len(aloop)==0:  # all gone
                       self.remove_loop(aloop)
                    break
        else:
          del self.block[real_keys[k_index]]
          self.lower_keys.remove(testkey)
          # now remove the key in the order list
          for i in range(len(self.item_order)):
            if isinstance(self.item_order[i],StringTypes): #may be loop
                if self.item_order[i].lower()==testkey:
                    del self.item_order[i]
                    break
	if len(self.block)==0:    #no items in loop, length -> 0
	    self.no_packets = 0
	return        #no duplicates, no more checking needed

@ Adding a data item.  This routine adds a single data item to a
pre-existing loop, checking both the dimension and length to make
sure they match already-existing items.  We make a special exception
for an empty list on the assumption that it is going to be filled
manually (in particular, using load_iter during file reading).

If an item is already stored, it will be silently replaced.  Note that
we can only guarantee this behaviour, and that duplicate items are not
present, if this is called in the top loop.  If it is called as a
method of an inner loop, only subloops are visible for checking/
replacing.  We could get around this restriction by being passed a
function which would fix things up for us.

We also check for consistency, by making sure the new item is
not in the block already.  If it is, we replace it (consistent with
the meaning of square brackets in Python), unless [[self.overwrite]]  
is False, in which case an error is raised.

We skip checking of data values if the [[precheck]] value is true- this
is typically set if the item is being read from a file, and so is already
checked.

Note the confusing behaviour to allow recursive calls at any depth of loop (STAR
has arbitrary depth).  If called with (string-valued key, value) then the
structure is ultimately updated with key:value, where value must be of the
appropriate dimension (0 for top-level).  If called with a compound
key and compound value, then this routine is recursively called with
zip(key,value).  As an exception, if called with a bare string key but a
compound datavalue of length 1

<<Add a data item>>=
def AddLoopItem(self,incomingdata,precheck=False,maxlength=-1):
    # print "Received data %s" % `incomingdata`
    # we accept tuples, strings, lists and dicts!!
    # Direct insertion: we have a string-valued key, with an array
    # of values -> single-item into our loop
    if isinstance(incomingdata[0],(TupleType,ListType)):
       # internal loop
       # first we remove any occurences of these datanames in
       # other loops
       for one_item in incomingdata[0]:
	   if self.has_key(one_item):
	       if not self.overwrite:
                   raise StarError( 'Attempt to insert duplicate item name %s' % incomingdata[0])
               else:
		   del self[one_item]
       newloop = self.loopclass(dimension = self.dimension+1,characterset=self.characterset)
       keyvals = zip(incomingdata[0],incomingdata[1])
       for key,val in keyvals:
           newloop.AddLoopItem((key,val))
       self.insert_loop(newloop)
    elif not isinstance(incomingdata[0],StringTypes):
         raise TypeError, 'Star datanames are strings only (got %s)' % `incomingdata[0]`
    else:
       data = list(incomingdata)  #copy
       if data[1] == [] or get_dim(data[1])[0] == self.dimension:
           if not precheck:
               self.check_data_name(data[0],maxlength)    # make sure no nasty characters   
           # check that we can replace data
           if not self.overwrite:
               if self.has_key(data[0]):
                   raise StarError( 'Attempt to insert duplicate item name %s' % data[0])
           # now make sure the data is OK type
           regval = self.regularise_data(data[1])
           if not precheck:
               self.check_item_value(regval)
           if self.dimension > 0:
               if self.no_packets <= 0:
                   self.no_packets = len(data[1])  #first item in this loop
               if len(data[1]) != self.no_packets:
                   raise StarLengthError, 'Not enough values supplied for %s' % (data[0])
           try:
               oldpos = self.GetItemPosition(data[0])
           except ValueError:
               oldpos = len(self.item_order)#end of list 
           self.RemoveLoopItem(data[0])     # may be different case, so have to do this
           self.block.update({data[0]:regval})  # trust the data is OK
           self.lower_keys.insert(oldpos,data[0].lower())
           self.item_order.insert(oldpos,data[0])
           #    self.lower_keys.append(data[0].lower())
           #    self.item_order.append(data[0])
            
       else:            #dimension mismatch
           # single-member lists could be seen as bare lists...
           if isinstance(data[1],(TupleType,ListType)) and len(data[1])==1:
               self.AddLoopItem(data[0],data[1][0])
           # if that doesn't work, make the dataname list a compound item for inserting a loop
           else:
               self.AddLoopItem(((data[0],),(data[1],)))
           # raise StarLengthError, "input data dim %d != required dim %d: %s %s" % (get_dim(data[1])[0],self.dimension,data[0],`data[1]`)

@ Checking the data names.  The CIF 1.1 standard restricts characters in
a data name to ASCII 33-126 and there should be a leading underscore.  Items
are allowed to have the blank characters as well, i.e. ascii 09,10,13 and 32.
Data items may be lists, which we need to detect before checking.  We assume
that the item has been regularised before this check is called.

The CIF2 standard allows all of Unicode, with certain blocks disallowed. The
removal of the disallowed characters takes place on file read. 

<<Check data name for STAR conformance>>=
def check_data_name(self,dataname,maxlength=-1): 
    if maxlength > 0:
        if len(dataname)>maxlength:
            raise StarError( 'Dataname %s exceeds maximum length %d' % (dataname,maxlength))
    if dataname[0]!='_':
        raise StarError( 'Dataname ' + dataname + ' does not begin with _')
    if self.characterset=='ascii':
        if len (filter (lambda a: ord(a) < 33 or ord(a) > 126, dataname)) > 0:
            raise StarError( 'Dataname ' + dataname + ' contains forbidden characters')
    else:
        # print 'Checking %s for unicode characterset conformance' % dataname
        if len (filter (lambda a: ord(a) < 33, dataname)) > 0:
            raise StarError( 'Dataname ' + dataname + ' contains forbidden characters (below code point 33)')
        if len (filter (lambda a: ord(a) > 126 and ord(a) < 160, dataname)) > 0:
            raise StarError( 'Dataname ' + dataname + ' contains forbidden characters (between code point 127-159)')
        if len (filter (lambda a: ord(a) > 0xD7FF and ord(a) < 0xE000, dataname)) > 0:
            raise StarError( 'Dataname ' + dataname + ' contains unsupported characters (between U+D800 and U+E000)')
        if len (filter (lambda a: ord(a) > 0xFDCF and ord(a) < 0xFDF0, dataname)) > 0:
            raise StarError( 'Dataname ' + dataname + ' contains unsupported characters (between U+FDD0 and U+FDEF)')
        if len (filter (lambda a: ord(a) == 0xFFFE or ord(a) == 0xFFFF, dataname)) > 0:
            raise StarError( 'Dataname ' + dataname + ' contains unsupported characters (U+FFFE and/or U+FFFF)')
        if len (filter (lambda a: ord(a) > 0x10000 and (ord(a) & 0xE == 0xE) , dataname)) > 0:
            print '%s fails' % dataname
            for a in dataname: print '%x' % ord(a),
            print
            raise StarError( u'Dataname ' + dataname + u' contains unsupported characters (U+xFFFE and/or U+xFFFF)')

<<Check data item for STAR conformance>>=
def check_item_value(self,item):
    test_item = item
    if type(item) != TupleType and type(item) != ListType and type(item) != DictType:
       test_item = [item]         #single item list
    def check_one (it):
        if type(it) in StringTypes:
            if it=='': return
            me = self.char_check.match(it)            
            if not me:
                print "Fail value check: %s" % it
                raise StarError, u'Bad character in %s' % it
            else:
                if me.span() != (0,len(it)):
                    print "Fail value check, match only %d-%d in string %s" % (me.span()[0],me.span()[1],it)
                    raise StarError,u'Data item "' + `it` +  u'"... contains forbidden characters'
    map(check_one,test_item)

@ Regularising data.  We want the copy.deepcopy operation to work, so
we can't have any arrays passed into the master dictionary.  We make
sure everything goes in either as a single item or as a dict/list/tuple.

<<Regularise data values>>=
def regularise_data(self,dataitem):
    alrighttypes = [IntType, LongType, 
                    FloatType, StringType, UnicodeType]
    okmappingtypes = [TupleType, ListType, DictType]
    thistype = type(dataitem)
    if thistype in alrighttypes or thistype in okmappingtypes:
        return dataitem
    if isinstance(dataitem,StarList) or \
       isinstance(dataitem,StarDict):
        return dataitem
    # so try to make into a list
    try:
        regval = list(dataitem)
    except TypeError, value:
        raise StarError( str(dataitem) + ' is wrong type for data value\n' )
    return regval
    
@ Dimension of data.  This would ordinarily be the number of nested levels, 
and if we have a naked string, we have to return zero.  
We recursively burrow down to the lowest level.  If a list is of zero length,
we can't burrow any further, so simply return one more than the current level.

We return as well the length of the received packet.  Note that we consider
dataitems which are *not* tuples or lists to be primitive.  This includes
StarLists (which are a single data item) and numpy arrays.  Unfortunately
this means we have to use the ungainly check involving the __class__ property,
as StarLists and Tuples are subclasses of list and tuple and will therefore
count as instances of them.  In the context of DDLm it is probably more
elegant to define a special class for looped data rather than for primitive
lists as data items.

This is a method of the module, rather than belonging to any particular class.

<<Get data dimension>>=
def get_dim(dataitem,current=0,packlen=0):
    zerotypes = [IntType, LongType, 
                    FloatType, StringType, UnicodeType]
    if type(dataitem) in zerotypes:
        return current, packlen
    if not dataitem.__class__ == ().__class__ and \
       not dataitem.__class__ == [].__class__:
       return current, packlen
    elif len(dataitem)>0: 
    #    print "Get_dim: %d: %s" % (current,`dataitem`)
        return get_dim(dataitem[0],current+1,len(dataitem))
    else: return current+1,0
    
@ The philosophy of outputting strings is to create a StringIO
object, and pass this between all the routines.  As there are
specific rules about when a new line can occur (especially
concerning semicolon-delimited strings) we subclass StringIO
and fiddle with the write method.

<<Functions for printing out>>=
<<Set the output length>>
<<Print a loop block>>
<<Format loop names>>
<<Format loop packets>>
<<Format a single packet item>>
<<Format a string>>
<<Format a data value>>

@ We adjust the write method to intelligently output lines, taking
care with CIF/STAR rules for output.  We allow the caller to specify:
(1) a line break prior to output (e.g. for a new dataname)
(2) a tab stepsize, in which case we try to pad out to this value
(3) that we can do a line break if we wish
(4) moving to a nested indent level, starting from the current position
(5) Whether or not to align the next item with the tab stops

We never insert newlines inside supplied strings.  Tabs are applied
after any requested line breaks, and both are applied before the next
item is output.

After adding any line breaks and/or tab stops, we recognise the following situations:
(1) The supplied string does not overflow the line: we output, and
update the length of the current line
(2) The supplied string does overflow the line.  
 (i) If we are allowed to break, we output a linefeed, and then the string.
 (ii) Otherwise, we output the string
(3) The supplied string contains linefeeds: we update the current line length according
to the number of characters from the beginning of the line.

<<Subclass StringIO>>=
from StringIO import StringIO
import math
class CIFStringIO(StringIO):
    def __init__(self,target_width=80,**kwargs):
        StringIO.__init__(self,**kwargs)
        self.currentpos = 0
        self.target_width = target_width
        self.tabwidth = -1
        self.indentlist = [0]

    def write(self,outstring,canbreak=False,mustbreak=False,do_tab=True,newindent=False,unindent=False):
        """Write a string with correct linebreak, tabs and indents"""
        # do we need to break?
        if mustbreak:    #insert a new line and indent
            StringIO.write(self,'\n' + ' '*self.indentlist[-1])
            self.currentpos = self.indentlist[-1]
        if self.currentpos+len(outstring)>self.target_width: #try to break
            if canbreak:
                StringIO.write(self,'\n'+' '*self.indentlist[-1])
                self.currentpos = self.indentlist[-1]
        if newindent:           #indent by current amount
            if self.indentlist[-1] == 0:    #first time
                self.indentlist.append(self.currentpos)
                print 'Indentlist: ' + `self.indentlist`
            else:
                self.indentlist.append(self.indentlist[-1]+2)
        elif unindent:
            if len(self.indentlist)>1:
                self.indentlist.pop()
            else:
                print 'Warning: cannot unindent any further'
        #handle tabs
        if self.tabwidth >0 and do_tab:
            next_stop = ((self.currentpos//self.tabwidth)+1)*self.tabwidth
            #print 'Currentpos %d: Next tab stop at %d' % (self.currentpos,next_stop)
            if self.currentpos < next_stop:
                StringIO.write(self,(next_stop-self.currentpos)*' ')
                self.currentpos = next_stop
        #now output the string
        StringIO.write(self,outstring)
        last_line_break = outstring.rfind('\n')
        if last_line_break >=0:
            self.currentpos = len(outstring)-last_line_break
        else:
            self.currentpos = self.currentpos + len(outstring)
        
    def set_tab(self,tabwidth):
        """Set the tab stop position"""
        self.tabwidth = tabwidth

@ For non-default output lengths, we include a function which will set
the internal attribute that controls maximum line length.  As this is
a per-block value, this function is most likely called by the StarFile
object rather than directly.

Two values control output line formatting: [[self.wraplength]] and 
[[self.maxoutlength]].  [[self.wraplength]] is the value at which the
line will be wrapped normally, but long strings will not force an
internal wrap inside the string; [[self.maxoutlength]] is the absolute
maximum length.
 
<<Set the output length>>=
def SetOutputLength(self,wraplength=80,maxoutlength=2048):
    if wraplength > maxoutlength:
        raise StarError("Wrap length (requested %d) must be <= Maximum line length (requested %d)" % (wraplength,maxoutlength))
    self.wraplength = wraplength
    self.maxoutlength = maxoutlength
    for loop in self.loops:
        loop.SetOutputLength(wraplength,maxoutlength)

@ Printing a section.  We allow an optional order list to be given, in
case the caller wants to order things in some nice way.  By default, we
use the item_order dictionary item.  Naturally, looped items are grouped
together according to their relative order in the order list.

Note that we must be careful to add spaces between data items, especially
when formatting string loop data, where our string addition could get
quite hairy.  As we are doing so much concatenation, we use a stringIO
buffer to speed it up.

Also, it is conceivable that we print an internal loop without the
enclosing loop.  This means that we cannot assume that we find ourselves
with a nice simple one-dimensional array after selecting out the matrix
coordinate of our current packet.  Therefore, if we are not starting
out with a zero-dimensional block, we use the contents of coord to make
our choice for every non-specified dimension.

This routine should not be called recursively.

We attempt some nice formatting by printing non-packet items with an apparent
tab stop at 40 characters.

<<Print a loop block>>=
def printsection(self,instring='',blockstart="",blockend="",indent=0,coord=[]):
    import string
    # first make an ordering
    order = self.item_order[:]
    # now do it...
    if not instring:
        outstring = CIFStringIO(target_width=80)       # the returned string
    else:
        outstring = instring
    if not coord:
        coords = [0]*(self.dimension-1)
    else:
        coords = coord
    if(len(coords)<self.dimension-1):
        raise StarError("Not enough block packet coordinates to uniquely define data")
    # print loop delimiter
    outstring.write(blockstart,canbreak=True)
    while len(order)>0:
        # print "Order now: " + `order`
        itemname = order.pop(0)
        if self.dimension == 0:            # ie value next to tag
   	    if not isinstance(itemname,LoopBlock):  #no loop
               outstring.set_tab(40)
               itemvalue = self[itemname]
               outstring.write(itemname,mustbreak=True,do_tab=False)
               outstring.write(' ',canbreak=True,do_tab=False)    #space after itemname
               self.format_value(itemvalue,outstring)
            else:   # we are asked to print an internal loop block
                #first make sure we have sensible coords.  Length should be one
                #less than the current dimension
                outstring.set_tab(10)       #guess this is OK?
                outstring.write(' '*indent,mustbreak=True,do_tab=False); outstring.write('loop_\n',do_tab=False)
                itemname.format_names(outstring,indent+2)
                itemname.format_packets(outstring,coords,indent+2)
        else:   # we are a nested loop
            outstring.write(' '*indent,mustbreak=True,do_tab=False); outstring.write('loop_\n',do_tab=False)
	    self.format_names(outstring,indent+2)
	    self.format_packets(outstring,coords,indent+2)
    if instring: return   #inside a recursion
    else:
        returnstring = outstring.getvalue()
    outstring.close()
    return returnstring

@ Formatting a data value. Data values may be stored as strings, numbers or compound values.  We call this routine recursively to format data values.  We use [[compound]] to flag that we are an embedded compound value, so that
we do not insert a line break before the top-level compound delimiter.

<<Format a data value>>=
def format_value(self,itemvalue,stringsink,compound=False):
    """Format a Star data value"""
    if isinstance(itemvalue,StringTypes):  #need to sanitize
       stringsink.write(self._formatstring(itemvalue),canbreak = True)
    elif isinstance(itemvalue,StarList):
       stringsink.set_tab(0)
       stringsink.write('[',canbreak=True,newindent=True,mustbreak=compound)
       if len(itemvalue)>0:
           self.format_value(itemvalue[0],stringsink)
           for listval in itemvalue[1:]:
              print 'Formatting %s' % `listval`
              stringsink.write(', ',do_tab=False)
              self.format_value(listval,stringsink,compound=True)
       stringsink.write(']',unindent=True)
    elif isinstance(itemvalue,StarDict):
       stringsink.set_tab(0)
       stringsink.write('{',newindent=True,mustbreak=compound)  #start a new line inside
       items = itemvalue.items()
       if len(items)>0:
           stringsink.write("'"+items[0][0]+"'"+':',canbreak=True)
           self.format_value(items[0][1],stringsink)
           for key,value in items[1:]:
               stringsink.write(', ')
               stringsink.write("'"+key+"'"+":",canbreak=True)
               self.format_value(value,stringsink)   #never break between key and value
       stringsink.write('}',unindent=True)
    else: 
       stringsink.write(str(itemvalue),canbreak=True)   #numbers

@ Formatting a loop section.  We are passed an indent and destination string,
and are expected to append a list of item names to the string indented by
the indicated number of spaces.  If we have loops, we add those in too.

<<Format loop names>>=
def format_names(self,outstring,indent=0):
    temp_order = self.item_order[:]
    while len(temp_order)>0:
        itemname = temp_order.pop(0)
        if isinstance(itemname,StringTypes):  #(not loop)
            outstring.write(' ' * indent,do_tab=False) 
            outstring.write(itemname,do_tab=False)
            outstring.write("\n",do_tab=False)
        else:                                # a loop
            outstring.write(' ' * indent,do_tab=False) 
            outstring.write("loop_\n",do_tab=False)
            itemname.format_names(outstring,indent+2)
            outstring.write(" stop_\n",do_tab=False)

@ Formatting a loop packet.  We are passed an array of coordinates into
the required packet, of length dim - 1, and have to output the corresponding
values.  Our final packet will involve collecting the ith value of each 
item in our particular loop.  Note that we have to be careful with 
indentation, as the <return>; digraph must be recognised.

<<Format loop packets>>=
def format_packets(self,outstring,coordinates,indent=0):
   import cStringIO
   import string
   # get our current group of data
   # print 'Coords: %s' % `coordinates`
   alldata = map(lambda a:self.coord_to_group(a,coordinates),self.item_order)
   # print 'Alldata: %s' % `alldata`
   packet_data = apply(zip,alldata)
   # print 'Packet data: %s' % `packet_data`
   for position in range(len(packet_data)):
       for point in range(len(packet_data[position])):
           datapoint = packet_data[position][point]
           packstring = self.format_packet_item(datapoint,indent,outstring)
       outstring.write("\n",do_tab=False)
           
@ Formatting a single packet item - could be a nested packet!  If we have a
list of nested packets, we have to transpose first.  Note also that a nested
packet implies a STAR file, which means there are no line length restrictions.
We are therefore a bit sloppy with our checking against [[wraplength]] and
[[maxoutlength]].

<<Format a single packet item>>=
def format_packet_item(self,pack_item,indent,outstring):
    # print 'Formatting %s' % `pack_item`
    if isinstance(pack_item,(StringType,UnicodeType,IntType,FloatType,LongType,StarList,StarDict)):
       if isinstance(pack_item,StringTypes):
           outstring.write(self._formatstring(pack_item)) 
       else: 
           self.format_value(pack_item,outstring)
       outstring.write(' ',canbreak=True,do_tab=False)
    # Now, for each nested loop we call ourselves again
    else:               # a nested packet
       if not isinstance(pack_item[0],(ListType,TupleType)):  #base packet
           item_list = pack_item
       else:
           item_list = apply(zip,pack_item)
       for sub_item in item_list:
           outstring.write(' ' + self.format_packet_item(sub_item,indent,outstring),canbreak=True)
       # stop_ is not issued at the end of each innermost packet
       if isinstance(pack_item[0],(ListType,TupleType)):
           outstring.write(' stop_ ',canbreak=True)

@ Formatting a string.  We make sure that the length of the item value
is less than [[self.maxoutlength]], or else we should split them, and so on. We check the
value for terminators and impossible apostrophes and length, before
deciding whether to print it and the item on a single line.  We try to
respect carriage returns in the string, if the caller has tried to do
the formatting for us.  If we are not putting apostrophes around a
string, we make the first character a space, to avoid problems if the
first character of a line is a semicolon.

The STAR specification states that embedded quotes are allowed so long
as they are not followed by a space.  So if we find any quotes followed
by spaces we output a semicolon-terminated string to avoid too much
messing around.  This routine is called very often and could be 
improved.

We have to catch empty strings as well, which are legal.  Another gotcha 
concerns 'embedded' strings; if the datavalue begins with a quote, it
will be output verbatim (and misunderstood) unless spaces elsewhere
force quotation.

<<Format a string>>=
def _formatstring(self,instring):
    import string
    if len(instring)==0: return "''"
    if len(instring)< (self.maxoutlength-2) and '\n' not in instring and not ('"' in instring and '\'' in instring):
        if not ' ' in instring and not '\t' in instring and not '\v' \
          in instring and not '_' in instring and not ',' in instring \
          and not (instring[0]=="'" or instring[0]=='"'):                  # no blanks
            return instring
        if not "'" in instring:                                       #use apostrophes
            return "'%s'" % (instring)
        elif not "\"" in instring:
            return '"%s"' % (instring)
    # is a long one or one that needs semicolons due to carriage returns
    outstring = "\n;"
    # if there are returns in the string, try to work with them
    while 1:
        retin = string.find(instring,'\n')+1
        if retin < self.maxoutlength and retin > 0:      # honour this break
            outstring = outstring + instring[:retin]
            instring = instring[retin:]
        elif len(instring)<self.maxoutlength:            # finished
            outstring = outstring + instring + '\n;\n'
            break
        else:                             # find a space
            for letter in range(self.maxoutlength-1,self.wraplength-1,-1): 
                if instring[letter] in ' \t\f': break
            outstring = outstring + instring[:letter+1]
            outstring = outstring + '\n'
            instring = instring[letter+1:]            
    return outstring

@ \section{Star Block class}

A Star Block is simply a LoopBlock. Historically it was distinguished by
holding save frames, but this has been removed.

<<StarBlock class>>=
class StarBlock(LoopBlock):
    <<Adjust emulation of mapping type>>
    <<Merge with another block>>

@ A Star Block is a Loop Block which can hold save frames in the outermost
loop.   From version 4.05 we do not allow save frames to be set from within
the block; rather, an enclosing block collection should be created (e.g. a
Star File) and the save frame added to that block collection with the
'enclosing' StarBlock set as its parent. We catch the saves key and
print an error message to show deprecation.

<<Adjust emulation of mapping type>>=
def __getitem__(self,key):
    if key == "saves":
        raise StarError("""The saves key is deprecated. Access the save block from
the enclosing block collection (e.g. CIF or STAR file)""")
    else:
        return LoopBlock.__getitem__(self,key)

def __setitem__(self,key,value):
    if key == "saves":
        raise StarError("""Setting the saves key is deprecated. Add the save block to
an enclosing block collection (e.g. CIF or STAR file) with this block as child""")
    else:
        LoopBlock.__setitem__(self,key,value)

def copy(self):
    newblock = LoopBlock.copy(self)
    return self.copy.im_class(newblock)   #catch inheritance

@ Merging.  Normally merging of dictionaries is done at the data file 
level, i.e. a whole block is replaced or added.  However, in 'overlay'
mode, individual keys are added/replaced, which is a block level
operation.  Furthermore, this routine is called when operating on
DDL2 dictionaries as they use save frames, which are
block internal, so all modes have to be supported.

Looped item overlaps are tricky.  We distinguish two cases: at least
one key in common, and all keys in common.  The latter implies 
addition of rows only.  The former implies deletion of all co-occuring
looped items (as they will otherwise have data of different lengths) and
therefore either completely replacing the previous item, or adding
the new data to the end, and including the other co-looped items.  But
this would mean that we were passed a loop block with different data
lengths in the new object, which is illegal, so we can only add to the
end if the new dictionary contains a subset of the attributes in the
current dictionary.  Therefore we have the following rules

(1) Identical attributes in new and old -> append
(2) New contains subset of old -> append values for common items and 
    delete extra looped items
(3) Old contains subset of new -> new completely replaces old

The [[match_att]] keyword is used when old and new blocks have been matched
based on an internal attribute (usually _name or _item.name).  This
attribute should not become looped in overlay mode, obviously, so we
need to have a record of it just in case.

The rel_keys keyword contains a list of datanames which act as unique keys (in a database
sense) inside loop structures.  If any keys match in separate datablocks, the row will
not be added, but simply replaced.

<<Merge with another block>>=
def merge(self,new_block,mode="strict",match_att=[],match_function=None,
               rel_keys = []):
    if mode == 'strict':
       for key in new_block.item_order: 
           if self.has_key(key) and key not in match_att:
              raise CifError( "Identical keys %s in strict merge mode" % key)
           elif key not in match_att:           #no change otherwise
              if isinstance(key,StringTypes):
                  self[key] = new_block[key] 
              else:
                  self.insert_loop(key)
    elif mode == 'replace':
       newkeys = new_block.keys()
       for ma in match_att:
          try:
               newkeys.remove(ma)        #don't touch the special ones
          except ValueError:
               pass
       for key in new_block.item_order: 
              if isinstance(key,StringTypes):
                  self[key] = new_block[key] 
              else:
                  self.insert_loop(key)   #assume is a loop
    elif mode == 'overlay':
       print 'Overlay mode, current overwrite is %s' % self.overwrite
       save_overwrite = self.overwrite
       self.overwrite = True
       for attribute in new_block.keys():
           if attribute in match_att: continue      #ignore this one
           new_value = new_block[attribute]
           #non-looped items
           if isinstance(new_value,StringTypes):
              self[attribute] = new_value 
       these_atts = self.keys()
       for newloop in new_block.loops:              
           newkeys = newloop.keys()
           # note that the following line determines packet item order
           overlaps = filter(lambda a: a in these_atts,newkeys)
           if len(overlaps)< len(newloop):#completely new loop
              self.insert_loop(newloop)
           elif len(overlaps)==len(newloop):
              # appending packets 
              # print "In overlay merge mode, found extra packet items:"
              # print `overlaps`
              # get key position
              loop_keys = filter(lambda a:a in rel_keys,overlaps)
              try:
                 newkeypos = map(lambda a:newkeys.index(a),loop_keys)
                 newkeypos = newkeypos[0]      #one key per loop for now
                 loop_keys = loop_keys[0] 
              except (ValueError,IndexError):
                 newkeypos = []
              overlap_data = map(lambda a:listify(self[a]),overlaps) #old packet data
              new_data = map(lambda a:new_block[a],overlaps) #new packet data
              packet_data = transpose(overlap_data)
              new_p_data = transpose(new_data)
              # remove any packets for which the keys match between old and new; we
              # make the arbitrary choice that the old data stays
              if newkeypos:
                  # get matching values in new list
                  print "Old, new data:\n%s\n%s" % (`overlap_data[newkeypos]`,`new_data[newkeypos]`)
                  key_matches = filter(lambda a:a in overlap_data[newkeypos],new_data[newkeypos])
                  # filter out any new data with these key values
                  new_p_data = filter(lambda a:a[newkeypos] not in key_matches,new_p_data)
                  if new_p_data:
                      new_data = transpose(new_p_data)
                  else: new_data = []
              # wipe out the old data and enter the new stuff
              byebyeloop = self.GetLoop(overlaps[0])
              # print "Removing '%s' with overlaps '%s'" % (`byebyeloop`,`overlaps`)
              # Note that if, in the original dictionary, overlaps are not
              # looped, GetLoop will return the block itself.  So we check
              # for this case...
              if byebyeloop != self:
                  self.remove_loop(byebyeloop)
              self.AddLoopItem((overlaps,overlap_data))  #adding old packets
              for pd in new_p_data:                             #adding new packets
                 if pd not in packet_data:
                    for i in range(len(overlaps)):
                        #don't do this at home; we are appending
                        #to something in place
                        self[overlaps[i]].append(pd[i]) 
       self.overwrite = save_overwrite

<<Define an error class>>=
class StarError(Exception):
    def __init__(self,value):
        self.value = value
    def __str__(self):
        return '\nStar Format error: '+ self.value 

class StarLengthError(Exception):
    def __init__(self,value):
        self.value = value
    def __str__(self):
        return '\nStar length error: ' + self.value

@ Listify - used to allow uniform treatment of datanames - otherwise sequence functions might
operate on a string instead of a list.

<<Utility functions>>=
def listify(item):
    if isinstance(item,StringTypes): return [item]
    else: return item

#Transpose the list of lists passed to us
def transpose(base_list):
    new_lofl = []
    full_length = len(base_list)
    opt_range = range(full_length)
    for i in range(len(base_list[0])):
       new_packet = [] 
       for j in opt_range:
          new_packet.append(base_list[j][i])
       new_lofl.append(new_packet)
    return new_lofl

